{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ],
      "metadata": {
        "id": "HO1J5_8OVfiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RjFSQsrU744",
        "outputId": "22f5fc14-f885-4c39-f13e-e956cb23b9d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu\n",
            "To: /content/Auto_MPG_data.csv\n",
            "\r  0% 0.00/15.4k [00:00<?, ?B/s]\r100% 15.4k/15.4k [00:00<00:00, 54.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "24tVT2EtVikD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rp2hZiU8U4gC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set fixed random seed"
      ],
      "metadata": {
        "id": "TpfdYZf6WJcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 59\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(random_state)"
      ],
      "metadata": {
        "id": "lYDTEV4wWwBB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up computing device"
      ],
      "metadata": {
        "id": "ts2upAG4XKR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "JPrF8N91XTNI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "esxpSLvzXZN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/Auto_MPG_data.csv'\n",
        "dataset = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "2QPT_GzDXkxq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing dataset"
      ],
      "metadata": {
        "id": "F-Le7npaX3ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split X feature and y label"
      ],
      "metadata": {
        "id": "EQvobCNgX8N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset.drop(columns='MPG').values\n",
        "y = dataset['MPG'].values"
      ],
      "metadata": {
        "id": "HmcKPpTJYFAA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split train/val/test"
      ],
      "metadata": {
        "id": "JhXkfBVFYORx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "8-jpRmVCYRu5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing input features"
      ],
      "metadata": {
        "id": "nn7pxJ-tY-od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = StandardScaler()\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "X_val = normalizer.transform(X_val)\n",
        "X_test = normalizer.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "VDsxMSAvZDDr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the DataLoader"
      ],
      "metadata": {
        "id": "v0TKxAl1Zb0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "ChZAOFODZezW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False)"
      ],
      "metadata": {
        "id": "7ipcXhgXZ09f"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building MLP network"
      ],
      "metadata": {
        "id": "Zxfjrx_5aW-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
        "    self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "    self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = F.relu(x)\n",
        "    out = self.output(x)\n",
        "\n",
        "    return out.squeeze(1)"
      ],
      "metadata": {
        "id": "GBbj1zDCaaam"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = X_train.shape[1]\n",
        "output_dims = 1\n",
        "hidden_dims = 64\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)"
      ],
      "metadata": {
        "id": "kfsSWEvTbH2j"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare loss function and optimizer"
      ],
      "metadata": {
        "id": "USTMb1tKbbFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "6aZJUCBKbjKz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bulding R2 score function"
      ],
      "metadata": {
        "id": "c9sFYc7Pbqou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r_squared(y_true, y_pred):\n",
        "  y_true = torch.Tensor(y_true).to(device)\n",
        "  y_pred = torch.Tensor(y_pred).to(device)\n",
        "  mean_true = torch.mean(y_true)\n",
        "  ss_total = torch.sum((y_true - mean_true) ** 2)\n",
        "  ss_res = torch.sum((y_true - y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res/ss_total)\n",
        "\n",
        "  return r2"
      ],
      "metadata": {
        "id": "pkYk5Rl7btJt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model"
      ],
      "metadata": {
        "id": "px23Ue76cLiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_r2 = []\n",
        "val_r2 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.0\n",
        "  train_target = []\n",
        "  val_target = []\n",
        "  train_predict = []\n",
        "  val_predict = []\n",
        "  model.train()\n",
        "\n",
        "  for X_samples, y_samples in train_loader:\n",
        "    X_samples = X_samples.to(device)\n",
        "    y_samples = y_samples.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_samples)\n",
        "    train_predict += outputs.tolist()\n",
        "    train_target += y_samples.tolist()\n",
        "    loss = criterion(outputs, y_samples)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  train_r2.append(r_squared(train_target, train_predict))\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_samples, y_samples in val_loader:\n",
        "      X_samples = X_samples.to(device)\n",
        "      y_samples = y_samples.to(device)\n",
        "      outputs = model(X_samples)\n",
        "      val_predict += outputs.tolist()\n",
        "      val_target += y_samples.tolist()\n",
        "      loss = criterion(outputs, y_samples)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_r2 = []\n",
        "val_r2 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.0\n",
        "  train_target = []\n",
        "  val_target = []\n",
        "  train_predict = []\n",
        "  val_predict = []\n",
        "  model.train()\n",
        "\n",
        "  for X_samples, y_samples in train_loader:\n",
        "    X_samples = X_samples.to(device)\n",
        "    y_samples = y_samples.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_samples)\n",
        "    train_predict += outputs.tolist()\n",
        "    train_target += y_samples.tolist()\n",
        "    loss = criterion(outputs, y_samples)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  train_r2.append(r_squared(train_target, train_predict))\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_samples, y_samples in val_loader:\n",
        "      X_samples = X_samples.to(device)\n",
        "      y_samples = y_samples.to(device)\n",
        "      outputs = model(X_samples)\n",
        "      val_predict += outputs.tolist()\n",
        "      val_target += y_samples.tolist()\n",
        "      loss = criterion(outputs, y_samples)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_r2.append(r_squared(val_target, val_predict))\n",
        "    print(f'\\nEPOCH {epoch+1}: \\t Training loss: {train_loss:.3f}\\t Validation loss: {val_loss:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48pqt8Q3cOJ-",
        "outputId": "4da74968-d507-49b2-aca5-1ad58600dc75"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1: \t Training loss: 5.541\t Validation loss: 31.191\n",
            "\n",
            "EPOCH 2: \t Training loss: 11.539\t Validation loss: 4.913\n",
            "\n",
            "EPOCH 3: \t Training loss: 5.414\t Validation loss: 4.508\n",
            "\n",
            "EPOCH 4: \t Training loss: 5.265\t Validation loss: 13.841\n",
            "\n",
            "EPOCH 5: \t Training loss: 7.851\t Validation loss: 5.260\n",
            "\n",
            "EPOCH 6: \t Training loss: 5.706\t Validation loss: 4.380\n",
            "\n",
            "EPOCH 7: \t Training loss: 4.466\t Validation loss: 5.553\n",
            "\n",
            "EPOCH 8: \t Training loss: 6.830\t Validation loss: 6.446\n",
            "\n",
            "EPOCH 9: \t Training loss: 6.447\t Validation loss: 6.575\n",
            "\n",
            "EPOCH 10: \t Training loss: 6.139\t Validation loss: 16.582\n",
            "\n",
            "EPOCH 11: \t Training loss: 5.617\t Validation loss: 7.986\n",
            "\n",
            "EPOCH 12: \t Training loss: 4.594\t Validation loss: 5.643\n",
            "\n",
            "EPOCH 13: \t Training loss: 5.586\t Validation loss: 6.282\n",
            "\n",
            "EPOCH 14: \t Training loss: 7.652\t Validation loss: 8.013\n",
            "\n",
            "EPOCH 15: \t Training loss: 9.135\t Validation loss: 8.246\n",
            "\n",
            "EPOCH 16: \t Training loss: 5.909\t Validation loss: 8.748\n",
            "\n",
            "EPOCH 17: \t Training loss: 8.293\t Validation loss: 7.667\n",
            "\n",
            "EPOCH 18: \t Training loss: 5.225\t Validation loss: 6.657\n",
            "\n",
            "EPOCH 19: \t Training loss: 6.363\t Validation loss: 6.456\n",
            "\n",
            "EPOCH 20: \t Training loss: 6.048\t Validation loss: 6.347\n",
            "\n",
            "EPOCH 21: \t Training loss: 4.583\t Validation loss: 4.639\n",
            "\n",
            "EPOCH 22: \t Training loss: 5.258\t Validation loss: 20.376\n",
            "\n",
            "EPOCH 23: \t Training loss: 9.206\t Validation loss: 9.995\n",
            "\n",
            "EPOCH 24: \t Training loss: 6.091\t Validation loss: 9.474\n",
            "\n",
            "EPOCH 25: \t Training loss: 5.000\t Validation loss: 5.032\n",
            "\n",
            "EPOCH 26: \t Training loss: 4.954\t Validation loss: 6.371\n",
            "\n",
            "EPOCH 27: \t Training loss: 5.472\t Validation loss: 7.539\n",
            "\n",
            "EPOCH 28: \t Training loss: 4.771\t Validation loss: 5.246\n",
            "\n",
            "EPOCH 29: \t Training loss: 5.864\t Validation loss: 6.052\n",
            "\n",
            "EPOCH 30: \t Training loss: 6.454\t Validation loss: 8.065\n",
            "\n",
            "EPOCH 31: \t Training loss: 7.951\t Validation loss: 5.203\n",
            "\n",
            "EPOCH 32: \t Training loss: 5.981\t Validation loss: 5.410\n",
            "\n",
            "EPOCH 33: \t Training loss: 5.812\t Validation loss: 8.745\n",
            "\n",
            "EPOCH 34: \t Training loss: 5.876\t Validation loss: 10.201\n",
            "\n",
            "EPOCH 35: \t Training loss: 9.338\t Validation loss: 9.561\n",
            "\n",
            "EPOCH 36: \t Training loss: 4.603\t Validation loss: 5.849\n",
            "\n",
            "EPOCH 37: \t Training loss: 6.162\t Validation loss: 8.970\n",
            "\n",
            "EPOCH 38: \t Training loss: 6.349\t Validation loss: 4.773\n",
            "\n",
            "EPOCH 39: \t Training loss: 7.420\t Validation loss: 14.639\n",
            "\n",
            "EPOCH 40: \t Training loss: 5.471\t Validation loss: 6.457\n",
            "\n",
            "EPOCH 41: \t Training loss: 4.549\t Validation loss: 5.140\n",
            "\n",
            "EPOCH 42: \t Training loss: 6.004\t Validation loss: 5.366\n",
            "\n",
            "EPOCH 43: \t Training loss: 6.088\t Validation loss: 10.295\n",
            "\n",
            "EPOCH 44: \t Training loss: 5.472\t Validation loss: 7.541\n",
            "\n",
            "EPOCH 45: \t Training loss: 9.923\t Validation loss: 9.844\n",
            "\n",
            "EPOCH 46: \t Training loss: 5.869\t Validation loss: 6.014\n",
            "\n",
            "EPOCH 47: \t Training loss: 5.128\t Validation loss: 5.677\n",
            "\n",
            "EPOCH 48: \t Training loss: 5.063\t Validation loss: 5.078\n",
            "\n",
            "EPOCH 49: \t Training loss: 6.586\t Validation loss: 8.585\n",
            "\n",
            "EPOCH 50: \t Training loss: 4.704\t Validation loss: 6.207\n",
            "\n",
            "EPOCH 51: \t Training loss: 6.124\t Validation loss: 5.168\n",
            "\n",
            "EPOCH 52: \t Training loss: 4.858\t Validation loss: 5.284\n",
            "\n",
            "EPOCH 53: \t Training loss: 4.538\t Validation loss: 5.592\n",
            "\n",
            "EPOCH 54: \t Training loss: 6.610\t Validation loss: 6.934\n",
            "\n",
            "EPOCH 55: \t Training loss: 4.674\t Validation loss: 5.026\n",
            "\n",
            "EPOCH 56: \t Training loss: 7.021\t Validation loss: 5.296\n",
            "\n",
            "EPOCH 57: \t Training loss: 7.560\t Validation loss: 4.295\n",
            "\n",
            "EPOCH 58: \t Training loss: 4.938\t Validation loss: 7.658\n",
            "\n",
            "EPOCH 59: \t Training loss: 4.763\t Validation loss: 4.409\n",
            "\n",
            "EPOCH 60: \t Training loss: 6.483\t Validation loss: 5.557\n",
            "\n",
            "EPOCH 61: \t Training loss: 4.467\t Validation loss: 7.126\n",
            "\n",
            "EPOCH 62: \t Training loss: 4.828\t Validation loss: 5.792\n",
            "\n",
            "EPOCH 63: \t Training loss: 4.736\t Validation loss: 4.731\n",
            "\n",
            "EPOCH 64: \t Training loss: 3.933\t Validation loss: 6.936\n",
            "\n",
            "EPOCH 65: \t Training loss: 6.527\t Validation loss: 7.390\n",
            "\n",
            "EPOCH 66: \t Training loss: 5.598\t Validation loss: 5.384\n",
            "\n",
            "EPOCH 67: \t Training loss: 4.361\t Validation loss: 11.020\n",
            "\n",
            "EPOCH 68: \t Training loss: 7.617\t Validation loss: 4.925\n",
            "\n",
            "EPOCH 69: \t Training loss: 5.633\t Validation loss: 13.914\n",
            "\n",
            "EPOCH 70: \t Training loss: 5.624\t Validation loss: 4.832\n",
            "\n",
            "EPOCH 71: \t Training loss: 4.156\t Validation loss: 27.270\n",
            "\n",
            "EPOCH 72: \t Training loss: 6.052\t Validation loss: 4.928\n",
            "\n",
            "EPOCH 73: \t Training loss: 3.848\t Validation loss: 5.688\n",
            "\n",
            "EPOCH 74: \t Training loss: 3.902\t Validation loss: 6.868\n",
            "\n",
            "EPOCH 75: \t Training loss: 6.354\t Validation loss: 11.306\n",
            "\n",
            "EPOCH 76: \t Training loss: 5.588\t Validation loss: 6.769\n",
            "\n",
            "EPOCH 77: \t Training loss: 4.401\t Validation loss: 4.856\n",
            "\n",
            "EPOCH 78: \t Training loss: 4.612\t Validation loss: 6.863\n",
            "\n",
            "EPOCH 79: \t Training loss: 7.239\t Validation loss: 11.501\n",
            "\n",
            "EPOCH 80: \t Training loss: 6.942\t Validation loss: 4.962\n",
            "\n",
            "EPOCH 81: \t Training loss: 3.894\t Validation loss: 4.936\n",
            "\n",
            "EPOCH 82: \t Training loss: 4.670\t Validation loss: 12.048\n",
            "\n",
            "EPOCH 83: \t Training loss: 7.844\t Validation loss: 7.556\n",
            "\n",
            "EPOCH 84: \t Training loss: 5.652\t Validation loss: 7.412\n",
            "\n",
            "EPOCH 85: \t Training loss: 4.299\t Validation loss: 5.295\n",
            "\n",
            "EPOCH 86: \t Training loss: 5.204\t Validation loss: 4.880\n",
            "\n",
            "EPOCH 87: \t Training loss: 4.355\t Validation loss: 6.657\n",
            "\n",
            "EPOCH 88: \t Training loss: 4.911\t Validation loss: 8.695\n",
            "\n",
            "EPOCH 89: \t Training loss: 8.624\t Validation loss: 6.053\n",
            "\n",
            "EPOCH 90: \t Training loss: 4.350\t Validation loss: 5.051\n",
            "\n",
            "EPOCH 91: \t Training loss: 4.373\t Validation loss: 6.039\n",
            "\n",
            "EPOCH 92: \t Training loss: 3.650\t Validation loss: 7.861\n",
            "\n",
            "EPOCH 93: \t Training loss: 5.077\t Validation loss: 6.133\n",
            "\n",
            "EPOCH 94: \t Training loss: 4.085\t Validation loss: 5.111\n",
            "\n",
            "EPOCH 95: \t Training loss: 3.986\t Validation loss: 7.678\n",
            "\n",
            "EPOCH 96: \t Training loss: 6.761\t Validation loss: 8.279\n",
            "\n",
            "EPOCH 97: \t Training loss: 3.856\t Validation loss: 5.267\n",
            "\n",
            "EPOCH 98: \t Training loss: 3.962\t Validation loss: 6.463\n",
            "\n",
            "EPOCH 99: \t Training loss: 4.949\t Validation loss: 6.258\n",
            "\n",
            "EPOCH 100: \t Training loss: 3.970\t Validation loss: 10.335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "DkcXNANAdpbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  X_test = X_test.to(device)\n",
        "  y_hat = model(X_test)\n",
        "  test_set_r2 = r_squared(y_hat, y_test)\n",
        "  print('Evaluation on test set:')\n",
        "  print(f'R2: {test_set_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq2iuWgsdrn0",
        "outputId": "252e8675-0efb-4d7d-b1e3-a9c0ae98cd07"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "R2: 0.8361618518829346\n"
          ]
        }
      ]
    }
  ]
}