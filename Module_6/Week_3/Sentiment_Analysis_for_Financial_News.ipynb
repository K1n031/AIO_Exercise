{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset and import libraries"
      ],
      "metadata": {
        "id": "RBra0X0fRNpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO5JSAlAREUZ",
        "outputId": "a985ddf5-e663-43f8-ebb9-df35a84d4090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uYXI4O3oWBA6QC8ZJ-r6yaTTfkdAnl_Q\n",
            "To: /content/dataset.zip\n",
            "\r  0% 0.00/230k [00:00<?, ?B/s]\r100% 230k/230k [00:00<00:00, 24.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1uYXI4O3oWBA6QC8ZJ-r6yaTTfkdAnl_Q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip './dataset.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W4ooY-dTD-u",
        "outputId": "eb46f36d-3fa1-4687-f834-0ccebfbf2dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/all-data.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode==0.04.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzXsRN5OSJTd",
        "outputId": "aa51cab9-44d1-4507-ce0b-cb1dab1574e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Unidecode==0.04.1\n",
            "  Downloading Unidecode-0.04.1.tar.gz (167 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/167.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Unidecode\n",
            "  Building wheel for Unidecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Unidecode: filename=Unidecode-0.4.1-py3-none-any.whl size=211976 sha256=32fc71a62ceee17c33f9c8fbc5c523f1b5ffcb8940ee8fd7ae906903458a4079\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/77/8c/1d8cef148e84ca19d365e7d1bad016fb0dc17e8eddddc53fbe\n",
            "Successfully built Unidecode\n",
            "Installing collected packages: Unidecode\n",
            "Successfully installed Unidecode-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from unidecode import unidecode\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZtWJtUSRP-F",
        "outputId": "40d2a6a5-c17a-4927-8484-8627d59e2358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read dataset"
      ],
      "metadata": {
        "id": "6mrww3C1RqOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = './dataset/all-data.csv'\n",
        "headers = ['sentiment', 'content']\n",
        "df = pd.read_csv(\n",
        "    dataset_path,\n",
        "    names=headers,\n",
        "    encoding='ISO-8859-1'\n",
        ")"
      ],
      "metadata": {
        "id": "vKwVawmIRu2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {\n",
        "    class_name: idx for idx, class_name in enumerate(df['sentiment'].unique().tolist())\n",
        "}\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])"
      ],
      "metadata": {
        "id": "W-myBFapTaxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing dataset"
      ],
      "metadata": {
        "id": "FnwPLuA4T3GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def text_normalize(text):\n",
        "  text = text.lower()\n",
        "  text = unidecode(text)\n",
        "  text = text.strip()\n",
        "  text = re.sub (r'[^\\w\\s]', '', text )\n",
        "  text = ' '.join([word for word in text.split(' ') if word not in english_stop_words])\n",
        "  text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "  return text\n",
        "\n",
        "df['content'] = df['content'].apply(lambda x: text_normalize(x))"
      ],
      "metadata": {
        "id": "wj7Gs9r5T4pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build vocabulary"
      ],
      "metadata": {
        "id": "IjQBfxn2U_Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for sentence in df['content'].tolist():\n",
        "  tokens = sentence.split()\n",
        "  for token in tokens:\n",
        "    if token not in vocab:\n",
        "      vocab.append(token)\n",
        "\n",
        "vocab.append('UNK')\n",
        "vocab.append('PAD')\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "86qCnyKgVC2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(text, word_to_idx, max_seq_len):\n",
        "  tokens = []\n",
        "  for w in text.split():\n",
        "    try:\n",
        "      tokens.append(word_to_idx[w])\n",
        "    except:\n",
        "      tokens.append(word_to_idx['UNK'])\n",
        "\n",
        "  if len(tokens) < max_seq_len:\n",
        "    tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n",
        "  elif len(tokens) > max_seq_len:\n",
        "    tokens = tokens[:max_seq_len]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "aQqNVC3mVhAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train, val, test"
      ],
      "metadata": {
        "id": "XFj9OgIQWZ-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "texts = df['content'].tolist()\n",
        "labels = df['sentiment'].tolist()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=val_size,\n",
        "    shuffle=is_shuffle,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size=test_size,\n",
        "    shuffle=is_shuffle,\n",
        "    random_state=seed\n",
        ")"
      ],
      "metadata": {
        "id": "Qm87pVFnWcZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build pytorch datasets"
      ],
      "metadata": {
        "id": "iMc_gqlVWyUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FinancialNews(Dataset):\n",
        "  def __init__(self, texts, labels, word_to_idx, max_seq_len, transform=None):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.word_to_idx = word_to_idx\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    if self.transform:\n",
        "      text = self.transform(text, self.word_to_idx, self.max_seq_len)\n",
        "\n",
        "    text = torch.tensor(text)\n",
        "\n",
        "    return text, label"
      ],
      "metadata": {
        "id": "GHN-5S3BWxN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declare dataloader"
      ],
      "metadata": {
        "id": "mE-CIIo8XcSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 32\n",
        "\n",
        "train_dataset = FinancialNews(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = FinancialNews(\n",
        "    X_val, y_val,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = FinancialNews(\n",
        "    X_test, y_test,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_batch_size = 128\n",
        "test_batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "KGVdwIhVXeQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ],
      "metadata": {
        "id": "DOE5X7FfYIkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_classes, dropout_prob):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.fc1 = nn.Linear(hidden_size, 16)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(16, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x, hn = self.rnn(x)\n",
        "    x = x[:, -1, :]\n",
        "    x = self.norm(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "fBhysn2eYKAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(list(classes.keys()))\n",
        "embedding_dim = 64\n",
        "hidden_size = 64\n",
        "n_layers = 2\n",
        "dropout_prob = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = SentimentClassifier(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    hidden_size,\n",
        "    n_layers,\n",
        "    n_classes,\n",
        "    dropout_prob\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "95WczwsSaIwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting loss and optimizer"
      ],
      "metadata": {
        "id": "Sd1v9ZqKb7d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "epochs = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "R_LWj1a0b9oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model"
      ],
      "metadata": {
        "id": "6Z9_5lfZcDPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    batch_train_losses = []\n",
        "    model.train()\n",
        "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_train_losses.append(loss.item())\n",
        "\n",
        "    train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'EPOCH {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}')\n",
        "\n",
        "  return train_losses, val_losses\n",
        "\n",
        "def evaluate(model, val_dataloader, criterion, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  losses = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  loss = sum(losses) / len(losses)\n",
        "  acc = correct / total\n",
        "\n",
        "  return loss, acc"
      ],
      "metadata": {
        "id": "UVoKkqvRcCky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device,\n",
        "    epochs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vpOk1iDciM5",
        "outputId": "4d78830a-44d5-4528-a070-2701b5a5760f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\tTrain loss: 0.9385\tVal loss: 0.9371\n",
            "EPOCH 2:\tTrain loss: 0.9324\tVal loss: 0.9350\n",
            "EPOCH 3:\tTrain loss: 0.9297\tVal loss: 0.9351\n",
            "EPOCH 4:\tTrain loss: 0.9312\tVal loss: 0.9349\n",
            "EPOCH 5:\tTrain loss: 0.9284\tVal loss: 0.9349\n",
            "EPOCH 6:\tTrain loss: 0.9280\tVal loss: 0.9349\n",
            "EPOCH 7:\tTrain loss: 0.9282\tVal loss: 0.9348\n",
            "EPOCH 8:\tTrain loss: 0.9290\tVal loss: 0.9348\n",
            "EPOCH 9:\tTrain loss: 0.9296\tVal loss: 0.9349\n",
            "EPOCH 10:\tTrain loss: 0.9300\tVal loss: 0.9349\n",
            "EPOCH 11:\tTrain loss: 0.9305\tVal loss: 0.9348\n",
            "EPOCH 12:\tTrain loss: 0.9302\tVal loss: 0.9349\n",
            "EPOCH 13:\tTrain loss: 0.9325\tVal loss: 0.9348\n",
            "EPOCH 14:\tTrain loss: 0.9286\tVal loss: 0.9348\n",
            "EPOCH 15:\tTrain loss: 0.9272\tVal loss: 0.9349\n",
            "EPOCH 16:\tTrain loss: 0.9282\tVal loss: 0.9348\n",
            "EPOCH 17:\tTrain loss: 0.9251\tVal loss: 0.9347\n",
            "EPOCH 18:\tTrain loss: 0.9277\tVal loss: 0.9348\n",
            "EPOCH 19:\tTrain loss: 0.9277\tVal loss: 0.9347\n",
            "EPOCH 20:\tTrain loss: 0.9251\tVal loss: 0.9346\n",
            "EPOCH 21:\tTrain loss: 0.9288\tVal loss: 0.9347\n",
            "EPOCH 22:\tTrain loss: 0.9287\tVal loss: 0.9348\n",
            "EPOCH 23:\tTrain loss: 0.9283\tVal loss: 0.9345\n",
            "EPOCH 24:\tTrain loss: 0.9263\tVal loss: 0.9346\n",
            "EPOCH 25:\tTrain loss: 0.9291\tVal loss: 0.9346\n",
            "EPOCH 26:\tTrain loss: 0.9255\tVal loss: 0.9345\n",
            "EPOCH 27:\tTrain loss: 0.9285\tVal loss: 0.9345\n",
            "EPOCH 28:\tTrain loss: 0.9267\tVal loss: 0.9343\n",
            "EPOCH 29:\tTrain loss: 0.9256\tVal loss: 0.9343\n",
            "EPOCH 30:\tTrain loss: 0.9242\tVal loss: 0.9342\n",
            "EPOCH 31:\tTrain loss: 0.9253\tVal loss: 0.9341\n",
            "EPOCH 32:\tTrain loss: 0.9268\tVal loss: 0.9339\n",
            "EPOCH 33:\tTrain loss: 0.9232\tVal loss: 0.9340\n",
            "EPOCH 34:\tTrain loss: 0.9242\tVal loss: 0.9336\n",
            "EPOCH 35:\tTrain loss: 0.9246\tVal loss: 0.9330\n",
            "EPOCH 36:\tTrain loss: 0.9247\tVal loss: 0.9317\n",
            "EPOCH 37:\tTrain loss: 0.9224\tVal loss: 0.9304\n",
            "EPOCH 38:\tTrain loss: 0.9231\tVal loss: 0.9287\n",
            "EPOCH 39:\tTrain loss: 0.9190\tVal loss: 0.9272\n",
            "EPOCH 40:\tTrain loss: 0.9197\tVal loss: 0.9318\n",
            "EPOCH 41:\tTrain loss: 0.9196\tVal loss: 0.9319\n",
            "EPOCH 42:\tTrain loss: 0.9155\tVal loss: 0.9304\n",
            "EPOCH 43:\tTrain loss: 0.9117\tVal loss: 0.9323\n",
            "EPOCH 44:\tTrain loss: 0.9128\tVal loss: 0.9332\n",
            "EPOCH 45:\tTrain loss: 0.9082\tVal loss: 0.9308\n",
            "EPOCH 46:\tTrain loss: 0.9038\tVal loss: 0.9311\n",
            "EPOCH 47:\tTrain loss: 0.8940\tVal loss: 0.9196\n",
            "EPOCH 48:\tTrain loss: 0.8793\tVal loss: 0.9338\n",
            "EPOCH 49:\tTrain loss: 0.9013\tVal loss: 0.9177\n",
            "EPOCH 50:\tTrain loss: 0.8829\tVal loss: 0.9091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
        "test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "print(\"Evaluation on val/test dataset\")\n",
        "print(f'Val acc: {val_acc:.4f}\\tVal loss: {val_loss:.4f}')\n",
        "print(f'Test acc: {test_acc:.4f}\\tTest loss: {test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfs-PE87eSc0",
        "outputId": "17f0d93a-c836-409d-e546-e4cfd52041e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on val/test dataset\n",
            "Val acc: 0.5948\tVal loss: 0.9091\n",
            "Test acc: 0.6186\tTest loss: 0.8765\n"
          ]
        }
      ]
    }
  ]
}